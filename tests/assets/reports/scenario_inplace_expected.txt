           Modules registered for optimization           
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓
┃ Module name ┃ Number of layers ┃ Number of parameters ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩
│ model_a     │                1 │                   10 │
│ model_b     │                2 │                   20 │
│ model_c     │                3 │                   30 │
└─────────────┴──────────────────┴──────────────────────┘
Optimizing: model_a
model_a: Collecting model information ...
model_a: Collecting model information OK
model_a: Building torchscript-script model from Torch model ...
model_a: Building torchscript-script model from Torch model OK
model_a: Building torchscript-trace model from Torch model ...
model_a: Building torchscript-trace model from Torch model OK
model_a: Building ExportedProgram from Torch model ...
model_a: Building ExportedProgram from Torch model OK
model_a: Building ONNX Trace model from Torch model ...
model_a: Building ONNX Trace model from Torch model OK
model_a: Optimizing graph for onnx ...
model_a: Optimizing graph for onnx OK
model_a: Finding max batch size for TensorRT ...
model_a: Finding max batch size for TensorRT OK
model_a: Building TensorRT engine trt-fp16 from ONNX model ...
model_a: Building TensorRT engine trt-fp16 from ONNX model OK
model_a: Validating model torch on TorchCUDA backend ...
model_a: Validating model torch on TorchCUDA backend OK
model_a: Validating model torchscript-script on TorchScriptCUDA backend ...
model_a: Validating model torchscript-script on TorchScriptCUDA backend OK
model_a: Validating model torchscript-trace on TorchScriptCUDA backend ...
model_a: Validating model torchscript-trace on TorchScriptCUDA backend OK
model_a: Validating model onnx on OnnxCUDA backend ...
model_a: Validating model onnx on OnnxCUDA backend OK
model_a: Validating model onnx on OnnxTensorRT backend ...
model_a: Validating model onnx on OnnxTensorRT backend OK
model_a: Validating model trt-fp16 on TensorRT backend ...
model_a: Validating model trt-fp16 on TensorRT backend OK
model_a: Benchmarking model torchscript-script on TorchScriptCUDA backend ...
model_a: Benchmarking model torchscript-script on TorchScriptCUDA backend OK
model_a: Benchmarking model torchscript-trace on TorchScriptCUDA backend ...
model_a: Benchmarking model torchscript-trace on TorchScriptCUDA backend OK
model_a: Benchmarking model onnx on OnnxCUDA backend ...
model_a: Benchmarking model onnx on OnnxCUDA backend OK
model_a: Benchmarking model onnx on OnnxTensorRT backend ...
model_a: Benchmarking model onnx on OnnxTensorRT backend OK
model_a: Benchmarking model trt-fp16 on TensorRT backend ...
model_a: Benchmarking model trt-fp16 on TensorRT backend OK
model_a: Benchmarking model torch on TorchCUDA backend ...
model_a: Benchmarking model torch on TorchCUDA backend OK
model_a: Verifying model torch on TorchCUDA backend ...
model_a: Verifying model torch on TorchCUDA backend OK
model_a: Verifying model torchscript-script on TorchScriptCUDA backend ...
model_a: Verifying model torchscript-script on TorchScriptCUDA backend OK
model_a: Verifying model torchscript-trace on TorchScriptCUDA backend ...
model_a: Verifying model torchscript-trace on TorchScriptCUDA backend OK
model_a: Verifying model onnx on OnnxCUDA backend ...
model_a: Verifying model onnx on OnnxCUDA backend OK
model_a: Verifying model onnx on OnnxTensorRT backend ...
model_a: Verifying model onnx on OnnxTensorRT backend OK
model_a: Verifying model trt-fp16 on TensorRT backend ...
model_a: Verifying model trt-fp16 on TensorRT backend OK
Optimizing: model_b
model_b: Collecting model information ...
model_b: Collecting model information OK
model_b: Building torchscript-script model from Torch model ...
model_b: Building torchscript-script model from Torch model OK
model_b: Building torchscript-trace model from Torch model ...
model_b: Building torchscript-trace model from Torch model OK
model_b: Building ExportedProgram from Torch model ...
model_b: Building ExportedProgram from Torch model OK
model_b: Building ONNX Trace model from Torch model ...
model_b: Building ONNX Trace model from Torch model OK
model_b: Optimizing graph for onnx ...
model_b: Optimizing graph for onnx OK
model_b: Finding max batch size for TensorRT ...
model_b: Finding max batch size for TensorRT OK
model_b: Building TensorRT engine trt-fp32 from ONNX model ...
model_b: Building TensorRT engine trt-fp32 from ONNX model OK
model_b: Building TensorRT engine trt-fp16 from ONNX model ...
model_b: Building TensorRT engine trt-fp16 from ONNX model OK
model_b: Validating model torch on TorchCUDA backend ...
model_b: Validating model torch on TorchCUDA backend OK
model_b: Validating model torchscript-script on TorchScriptCUDA backend ...
model_b: Validating model torchscript-script on TorchScriptCUDA backend OK
model_b: Validating model torchscript-trace on TorchScriptCUDA backend ...
model_b: Validating model torchscript-trace on TorchScriptCUDA backend OK
model_b: Validating model onnx on OnnxCUDA backend ...
model_b: Validating model onnx on OnnxCUDA backend OK
model_b: Validating model onnx on OnnxTensorRT backend ...
model_b: Validating model onnx on OnnxTensorRT backend OK
model_b: Validating model trt-fp32 on TensorRT backend ...
model_b: Validating model trt-fp32 on TensorRT backend OK
model_b: Validating model trt-fp16 on TensorRT backend ...
model_b: Validating model trt-fp16 on TensorRT backend OK
model_b: Benchmarking model torchscript-script on TorchScriptCUDA backend ...
model_b: Benchmarking model torchscript-script on TorchScriptCUDA backend OK
model_b: Benchmarking model torchscript-trace on TorchScriptCUDA backend ...
model_b: Benchmarking model torchscript-trace on TorchScriptCUDA backend OK
model_b: Benchmarking model onnx on OnnxCUDA backend ...
model_b: Benchmarking model onnx on OnnxCUDA backend OK
model_b: Benchmarking model onnx on OnnxTensorRT backend ...
model_b: Benchmarking model onnx on OnnxTensorRT backend OK
model_b: Benchmarking model trt-fp32 on TensorRT backend ...
model_b: Benchmarking model trt-fp32 on TensorRT backend OK
model_b: Benchmarking model trt-fp16 on TensorRT backend ...
model_b: Benchmarking model trt-fp16 on TensorRT backend OK
model_b: Benchmarking model torch on TorchCUDA backend ...
model_b: Benchmarking model torch on TorchCUDA backend OK
model_b: Verifying model torch on TorchCUDA backend ...
model_b: Verifying model torch on TorchCUDA backend OK
model_b: Verifying model torchscript-script on TorchScriptCUDA backend ...
model_b: Verifying model torchscript-script on TorchScriptCUDA backend OK
model_b: Verifying model torchscript-trace on TorchScriptCUDA backend ...
model_b: Verifying model torchscript-trace on TorchScriptCUDA backend OK
model_b: Verifying model onnx on OnnxCUDA backend ...
model_b: Verifying model onnx on OnnxCUDA backend OK
model_b: Verifying model onnx on OnnxTensorRT backend ...
model_b: Verifying model onnx on OnnxTensorRT backend OK
model_b: Verifying model trt-fp32 on TensorRT backend ...
model_b: Verifying model trt-fp32 on TensorRT backend OK
model_b: Verifying model trt-fp16 on TensorRT backend ...
model_b: Verifying model trt-fp16 on TensorRT backend OK
Optimizing: model_c
model_c: Collecting model information ...
model_c: Collecting model information OK
model_c: Building torchscript-script model from Torch model ...
model_c: Building torchscript-script model from Torch model OK
model_c: Building torchscript-trace model from Torch model ...
model_c: Building torchscript-trace model from Torch model OK
model_c: Building ExportedProgram from Torch model ...
model_c: Building ExportedProgram from Torch model OK
model_c: Building ONNX Trace model from Torch model ...
model_c: Building ONNX Trace model from Torch model OK
model_c: Optimizing graph for onnx ...
model_c: Optimizing graph for onnx OK
model_c: Finding max batch size for TensorRT ...
model_c: Finding max batch size for TensorRT OK
model_c: Building TensorRT engine trt-fp32 from ONNX model ...
model_c: Building TensorRT engine trt-fp32 from ONNX model OK
model_c: Building TensorRT engine trt-fp16 from ONNX model ...
model_c: Building TensorRT engine trt-fp16 from ONNX model OK
model_c: Validating model torch on TorchCUDA backend ...
model_c: Validating model torch on TorchCUDA backend OK
model_c: Validating model torchscript-script on TorchScriptCUDA backend ...
model_c: Validating model torchscript-script on TorchScriptCUDA backend OK
model_c: Validating model torchscript-trace on TorchScriptCUDA backend ...
model_c: Validating model torchscript-trace on TorchScriptCUDA backend OK
model_c: Validating model onnx on OnnxCUDA backend ...
model_c: Validating model onnx on OnnxCUDA backend OK
model_c: Validating model onnx on OnnxTensorRT backend ...
model_c: Validating model onnx on OnnxTensorRT backend OK
model_c: Validating model trt-fp32 on TensorRT backend ...
model_c: Validating model trt-fp32 on TensorRT backend OK
model_c: Validating model trt-fp16 on TensorRT backend ...
model_c: Validating model trt-fp16 on TensorRT backend OK
model_c: Benchmarking model torchscript-script on TorchScriptCUDA backend ...
model_c: Benchmarking model torchscript-script on TorchScriptCUDA backend OK
model_c: Benchmarking model torchscript-trace on TorchScriptCUDA backend ...
model_c: Benchmarking model torchscript-trace on TorchScriptCUDA backend OK
model_c: Benchmarking model onnx on OnnxCUDA backend ...
model_c: Benchmarking model onnx on OnnxCUDA backend OK
model_c: Benchmarking model onnx on OnnxTensorRT backend ...
model_c: Benchmarking model onnx on OnnxTensorRT backend OK
model_c: Benchmarking model trt-fp32 on TensorRT backend ...
model_c: Benchmarking model trt-fp32 on TensorRT backend OK
model_c: Benchmarking model trt-fp16 on TensorRT backend ...
model_c: Benchmarking model trt-fp16 on TensorRT backend OK
model_c: Benchmarking model torch on TorchCUDA backend ...
model_c: Benchmarking model torch on TorchCUDA backend OK
model_c: Verifying model torch on TorchCUDA backend ...
model_c: Verifying model torch on TorchCUDA backend OK
model_c: Verifying model torchscript-script on TorchScriptCUDA backend ...
model_c: Verifying model torchscript-script on TorchScriptCUDA backend OK
model_c: Verifying model torchscript-trace on TorchScriptCUDA backend ...
model_c: Verifying model torchscript-trace on TorchScriptCUDA backend OK
model_c: Verifying model onnx on OnnxCUDA backend ...
model_c: Verifying model onnx on OnnxCUDA backend OK
model_c: Verifying model onnx on OnnxTensorRT backend ...
model_c: Verifying model onnx on OnnxTensorRT backend OK
model_c: Verifying model trt-fp32 on TensorRT backend ...
model_c: Verifying model trt-fp32 on TensorRT backend OK
model_c: Verifying model trt-fp16 on TensorRT backend ...
model_c: Verifying model trt-fp16 on TensorRT backend OK
Optimization finished for all modules.
        Optimization result for max throughput and min latency strategy         
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Module name ┃ Optimized backend              ┃ Path                          ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ model_a     │ torchscript-trace on           │ /home/dev/.cache/model_navig… │
│             │ TorchScriptCUDA backend        │                               │
│ model_b     │ torchscript-trace on           │ /home/dev/.cache/model_navig… │
│             │ TorchScriptCUDA backend        │                               │
│ model_c     │ torchscript-trace on           │ /home/dev/.cache/model_navig… │
│             │ TorchScriptCUDA backend        │                               │
└─────────────┴────────────────────────────────┴───────────────────────────────┘
